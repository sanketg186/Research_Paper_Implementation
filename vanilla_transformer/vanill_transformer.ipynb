{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "import torch\n",
    "import math\n",
    "import bpemb\n",
    "# from torchvision.datasets import MNIST\n",
    "# from torchvision.transforms import ToTensor\n",
    "# import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.datasets import AG_NEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self,d_model=512,d_k=64,masking=False):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_model = d_model\n",
    "        self.masking =masking\n",
    "        self.W_Q = nn.Linear(in_features=d_model,out_features=d_k)\n",
    "        self.W_K = nn.Linear(in_features=d_model,out_features=d_k)\n",
    "        self.W_V = nn.Linear(in_features=d_model,out_features=d_k)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self,embedding):\n",
    "        q = self.W_Q(embedding)\n",
    "        k = self.W_K(embedding)\n",
    "        v = self.W_V(embedding)\n",
    "\n",
    "        if self.masking==False:\n",
    "            attention=self.softmax(torch.matmul(q,k.T)/math.sqrt(self.d_k))\n",
    "            z = torch.matmul(attention,v)\n",
    "            return z\n",
    "        else:\n",
    "            mask = torch.triu(torch.ones(q.size(0),k.size(0)), diagonal=1)\n",
    "            # Replace ones with -inf (disallowed positions)\n",
    "            mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "            scores = torch.matmul(q,k.T)/math.sqrt(self.d_k)\n",
    "            scores = scores.masked_fill(mask == 1, float('-inf'))\n",
    "            masked_attention=self.softmax(scores)\n",
    "            z = torch.matmul(masked_attention,v)\n",
    "            return z\n",
    "            # mask = torch.triu(torch.ones(q.size(0),k.size(0)), diagonal=1)\n",
    "            # # Replace ones with -inf (disallowed positions)\n",
    "            # mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "            # attention=self.softmax(torch.matmul(q,k.T)/math.sqrt(self.d_k))\n",
    "            # masked_attention = torch.add(attention,mask)\n",
    "            # z = torch.matmul(masked_attention,v)\n",
    "            # return z\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiHeaded Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=512, d_k=64, num_heads=8, dropout=0.1, masking=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k  ## assume d_v always equals d_k\n",
    "        self.masking = masking\n",
    "\n",
    "        self.scale_dotproduct_list= torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(self.num_heads):\n",
    "            self.scale_dotproduct_list.append(ScaledDotProductAttention(d_model=self.d_model,d_k=self.d_k,masking=self.masking))\n",
    "        \n",
    "        self.linear_layer = nn.Linear(in_features=self.num_heads*self.d_k,out_features=d_model)\n",
    "\n",
    "    \n",
    "    def forward(self,embedding):\n",
    "        z_list = []\n",
    "        for head_idx in range(self.num_heads):\n",
    "            z = self.scale_dotproduct_list[head_idx](embedding)\n",
    "            z_list.append(z)\n",
    "        \n",
    "        aggregate_z_output = torch.cat(z_list,dim=1)\n",
    "        final_z = self.linear_layer(aggregate_z_output)\n",
    "        return final_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,d_model,d_ff):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.fc1 = nn.Linear(in_features=d_model,out_features=d_ff)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(in_features=d_ff,out_features=d_model)\n",
    "    \n",
    "    def forward(self,embedding):\n",
    "        f1 = self.fc1(embedding)\n",
    "        f1_activation = self.gelu(f1)\n",
    "        f2 = self.fc2(f1_activation)\n",
    "        return f2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,d_model=512,d_k = 64, num_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.d_k = d_k\n",
    "        self.dropout = dropout\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model=d_model,d_k=d_k, num_heads=num_heads, dropout=dropout)\n",
    "        self.add_and_norm = nn.LayerNorm(normalized_shape=self.d_model)\n",
    "        self.feed_forward_layer = FeedForward(d_model, d_ff)\n",
    "\n",
    "    def forward(self,embedding):\n",
    "        z1 = self.add_and_norm(embedding + self.multi_head_attention(embedding))\n",
    "        z2 =self.add_and_norm(z1 + self.feed_forward_layer(z1))\n",
    "        return z2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,d_model=512, d_k=64, num_heads=8, d_ff=2048, dropout=0.1, num_encoder_blocks=6):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "        self.num_encoder_blocks = num_encoder_blocks\n",
    "        self.encoder_block_list = torch.nn.ModuleList()\n",
    "        for i in range(num_encoder_blocks):\n",
    "            self.encoder_block_list.append(EncoderBlock(d_model=d_model, d_k=d_k, num_heads=num_heads, d_ff=d_ff, dropout=dropout))\n",
    "        \n",
    "        self.W_K = nn.Linear(in_features=d_model,out_features=self.d_k)\n",
    "        self.W_V = nn.Linear(in_features=d_model,out_features=self.d_k)\n",
    "    \n",
    "    def forward(self,embedding):\n",
    "        encoder_out = embedding\n",
    "        for i in range(self.num_encoder_blocks):\n",
    "            encoder_out = self.encoder_block_list[i](encoder_out)\n",
    "        \n",
    "        k = self.W_K(encoder_out)\n",
    "        v = self.W_V(encoder_out)\n",
    "\n",
    "        return encoder_out,k,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttentionDecoder(nn.Module):\n",
    "    def __init__(self,d_model=512,d_k=64,masking=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.masking = masking\n",
    "        self.W_Q = nn.Linear(in_features=d_model,out_features=d_k)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self,embedding,k,v):\n",
    "        q = self.W_Q(embedding)\n",
    "        if self.masking==False:\n",
    "            attention=self.softmax(torch.matmul(q,k.T)/math.sqrt(self.d_k))\n",
    "            z = torch.matmul(attention,v)\n",
    "            return z\n",
    "        else:\n",
    "            mask = torch.triu(torch.ones(q.size(0),k.size(0)), diagonal=1)\n",
    "            # Replace ones with -inf (disallowed positions)\n",
    "            mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "            scores = torch.matmul(q,k.T)/math.sqrt(self.d_k)\n",
    "            scores = scores.masked_fill(mask == 1, float('-inf'))\n",
    "            masked_attention=self.softmax(scores)\n",
    "            # masked_attention = torch.add(attention,mask)\n",
    "            z = torch.matmul(masked_attention,v)\n",
    "\n",
    "            # print(\"sank:\",q.size(0),\" \",k.size(0))\n",
    "            # mask = torch.triu(torch.ones(q.size(0),k.size(0)), diagonal=1)\n",
    "            # # Replace ones with -inf (disallowed positions)\n",
    "            # mask = mask.masked_fill(mask == 1, float('-inf'))\n",
    "            # attention=self.softmax(torch.matmul(q,k.T)/math.sqrt(self.d_k))\n",
    "            # masked_attention = torch.add(attention,mask)\n",
    "            # z = torch.matmul(masked_attention,v)\n",
    "            return z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder decoder Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderAttention(nn.Module):\n",
    "    def __init__(self,d_model=512, d_k=64, num_heads=8, dropout=0.1,masking=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_k\n",
    "        self.dropout = dropout\n",
    "        self.masking = masking\n",
    "        \n",
    "        self.scale_dotproduct_list= torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(self.num_heads):\n",
    "            self.scale_dotproduct_list.append(ScaledDotProductAttentionDecoder(d_model=self.d_model,d_k=self.d_k,masking=self.masking))\n",
    "        \n",
    "        self.linear_layer = nn.Linear(in_features=self.num_heads*self.d_k,out_features=d_model)\n",
    "    \n",
    "    def forward(self, embedding,k,v):\n",
    "        z_list = []\n",
    "        for head_idx in range(self.num_heads):\n",
    "            z = self.scale_dotproduct_list[head_idx](embedding,k,v)\n",
    "            z_list.append(z)\n",
    "\n",
    "        aggregate_z_output = torch.cat(z_list,dim=1)\n",
    "        final_z = self.linear_layer(aggregate_z_output)\n",
    "        return final_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,d_model=512,d_k=64, num_heads=8, d_ff=2048, dropout=0.1,masking=False):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "        self.masking = masking\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(d_model,d_k=d_k, num_heads=num_heads, dropout=dropout,masking=masking)\n",
    "        self.encoder_decoder_attention = EncoderDecoderAttention(d_model=d_model,d_k=d_k,num_heads=num_heads,dropout=dropout,masking=masking)\n",
    "        self.add_and_norm = nn.LayerNorm(normalized_shape=self.d_model)\n",
    "        self.feed_forward_layer = FeedForward(d_model, d_ff)\n",
    "\n",
    "    def forward(self,embedding,k,v):\n",
    "        x = self.multi_head_attention(embedding)\n",
    "        z1 = self.add_and_norm(embedding + x)\n",
    "        z2 = self.add_and_norm(z1+self.encoder_decoder_attention(embedding,k,v))\n",
    "        z2 =self.add_and_norm(z2 + self.feed_forward_layer(z2))\n",
    "        return z2        \n",
    "    \n",
    "\n",
    "    # def forward(self,embedding,k,v):\n",
    "    #     z1 = self.add_and_norm(embedding + self.multi_head_attention(embedding))\n",
    "    #     print(\"sank:decoder block 1:\",z1)\n",
    "    #     z2 = self.add_and_norm(z1+self.encoder_decoder_attention(embedding,k,v))\n",
    "    #     print(\"sank:decoder block 2:\",z2)\n",
    "    #     z2 =self.add_and_norm(z2 + self.feed_forward_layer(z2))\n",
    "    #     print(\"sank:decoder block 3:\",z2)\n",
    "    #     return z2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model=512, d_k=64, num_heads=8, d_ff=2048, dropout=0.1, num_decoder_blocks=6,masking=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_k\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "        self.num_decoder_blocks = num_decoder_blocks\n",
    "        self.masking = masking\n",
    "        self.decoder_block_list = torch.nn.ModuleList()\n",
    "        for i in range(num_decoder_blocks):\n",
    "            self.decoder_block_list.append(DecoderBlock(d_model=d_model,d_k=d_k, num_heads=num_heads, d_ff=d_ff, dropout=dropout,masking=masking))\n",
    "        self.linear_layer = nn.Linear(in_features=d_model,out_features=vocab_size)\n",
    "        \n",
    "        self.W_K = nn.Linear(in_features=d_model,out_features=self.d_k)\n",
    "        self.W_V = nn.Linear(in_features=d_model,out_features=self.d_k)\n",
    "    \n",
    "    def forward(self,embedding,k,v):\n",
    "        decoder_out = embedding\n",
    "        for i in range(self.num_decoder_blocks):\n",
    "            decoder_out = self.decoder_block_list[i](decoder_out,k,v)\n",
    "        \n",
    "        decoder_out = self.linear_layer(decoder_out)\n",
    "\n",
    "        return decoder_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model=512, d_k=64, num_heads=8, d_ff=2048, dropout=0.1, num_encoder_blocks=6,num_decoder_blocks=6,masking=False):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.num_encoder_blocks = num_encoder_blocks\n",
    "        self.num_decoder_blocks = num_decoder_blocks\n",
    "        self.encoder = Encoder(d_model=d_model,d_k=d_k, num_heads=8, d_ff=d_ff, dropout=dropout, num_encoder_blocks=num_encoder_blocks)\n",
    "        self.decoder = Decoder(vocab_size,d_model=d_model, d_k=d_k, num_heads=num_heads, d_ff=d_ff, dropout=dropout, num_decoder_blocks=num_decoder_blocks,masking=masking)\n",
    "    \n",
    "    def forward(self,source_embedding,target_embedding):\n",
    "        _,k,v = self.encoder(source_embedding)\n",
    "        output = self.decoder(target_embedding,k,v)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encode(embedding,d_model):\n",
    "    seq_len = len(embedding)\n",
    "    positional_encoding = torch.zeros(seq_len, d_model)\n",
    "    positions = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)  # Shape: (seq_len, 1)\n",
    "    # Frequency scaling factors\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "    # Apply sine to even indices and cosine to odd indices\n",
    "    positional_encoding[:, 0::2] = torch.sin(positions * div_term)  # Even dimensions\n",
    "    positional_encoding[:, 1::2] = torch.cos(positions * div_term)  # Odd dimensions\n",
    "\n",
    "    return positional_encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Dataset Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerData(torch.utils.data.Dataset):\n",
    "    def __init__(self,source_language_file,target_language_file,source_language='en',target_language='de',source_vocab_size=200000,target_vocab_size=200000):\n",
    "        source_file = open(source_language_file,'r')\n",
    "        target_file = open(target_language_file,'r')\n",
    "        self.source_lines = source_file.readlines()\n",
    "        self.target_lines = target_file.readlines()\n",
    "        self.source_language = source_language\n",
    "        self.target_language = target_language\n",
    "        self.source_vocab_size = source_vocab_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.source_bpemb = bpemb.BPEmb(lang=source_language,vs=source_vocab_size)\n",
    "        self.target_bpemb = bpemb.BPEmb(lang=target_language,vs=target_vocab_size)\n",
    "        self.source_embed = torch.nn.Embedding.from_pretrained(torch.tensor(self.source_bpemb.vectors))\n",
    "        self.target_embed = torch.nn.Embedding.from_pretrained(torch.tensor(self.target_bpemb.vectors))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_lines)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        source = self.source_lines[index]\n",
    "        target = self.target_lines[index]\n",
    "        \n",
    "        source_tokens = self.source_bpemb.encode_ids(source)\n",
    "        source_tokens.insert(0,self.source_bpemb.BOS)\n",
    "        source_tokens.append(self.source_bpemb.EOS)\n",
    "\n",
    "        target_tokens = self.target_bpemb.encode_ids(target)\n",
    "        target_tokens.insert(0,self.target_bpemb.BOS)\n",
    "        target_tokens.append(self.target_bpemb.EOS)\n",
    "\n",
    "        source_line_embedding = self.source_embed(torch.tensor(source_tokens))\n",
    "        target_line_embedding = self.target_embed(torch.tensor(target_tokens))\n",
    "\n",
    "        embedding_dim_source = int(source_line_embedding[0].shape[0])\n",
    "        embedding_dim_target = int(target_line_embedding[0].shape[0])\n",
    "        \n",
    "        source_positional_encoding = positional_encode(source_line_embedding,embedding_dim_source)\n",
    "        target_positional_encoding = positional_encode(target_line_embedding,embedding_dim_target)\n",
    "\n",
    "        source_embedding_pe = torch.add(source_line_embedding, source_positional_encoding)\n",
    "        target_embedding_pe = torch.add(target_line_embedding, target_positional_encoding)\n",
    "\n",
    "        return source_embedding_pe,target_embedding_pe,target_tokens\n",
    "    \n",
    "    def return_bpemb_target_instance(self):\n",
    "        return self.target_bpemb\n",
    "\n",
    "    def return_embedding_layer_target(self):\n",
    "        return self.target_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transformer_dataset = TransformerData(source_language_file=\"train_en.txt\", target_language_file=\"train_de.txt\", source_language=\"en\", target_language=\"de\", source_vocab_size=200000, target_vocab_size=200000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = transformer_dataset[0][0].shape[1]\n",
    "vocab_size = 200000\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_encoder_blocks = 6\n",
    "num_decoder_blocks = 6\n",
    "masking = True\n",
    "d_k =64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(vocab_size,d_model=d_model,d_k=d_k, num_heads=8, d_ff=2048, dropout=0.1, \n",
    "                          num_encoder_blocks=num_encoder_blocks,num_decoder_blocks=num_decoder_blocks,masking=masking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer, \"simple_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (encoder_block_list): ModuleList(\n",
       "      (0-5): 6 x EncoderBlock(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (scale_dotproduct_list): ModuleList(\n",
       "            (0-7): 8 x ScaledDotProductAttention(\n",
       "              (W_Q): Linear(in_features=100, out_features=64, bias=True)\n",
       "              (W_K): Linear(in_features=100, out_features=64, bias=True)\n",
       "              (W_V): Linear(in_features=100, out_features=64, bias=True)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "          )\n",
       "          (linear_layer): Linear(in_features=512, out_features=100, bias=True)\n",
       "        )\n",
       "        (add_and_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward_layer): FeedForward(\n",
       "          (fc1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (W_K): Linear(in_features=100, out_features=64, bias=True)\n",
       "    (W_V): Linear(in_features=100, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder_block_list): ModuleList(\n",
       "      (0-5): 6 x DecoderBlock(\n",
       "        (multi_head_attention): MultiHeadAttention(\n",
       "          (scale_dotproduct_list): ModuleList(\n",
       "            (0-7): 8 x ScaledDotProductAttention(\n",
       "              (W_Q): Linear(in_features=100, out_features=64, bias=True)\n",
       "              (W_K): Linear(in_features=100, out_features=64, bias=True)\n",
       "              (W_V): Linear(in_features=100, out_features=64, bias=True)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "          )\n",
       "          (linear_layer): Linear(in_features=512, out_features=100, bias=True)\n",
       "        )\n",
       "        (encoder_decoder_attention): EncoderDecoderAttention(\n",
       "          (scale_dotproduct_list): ModuleList(\n",
       "            (0-7): 8 x ScaledDotProductAttentionDecoder(\n",
       "              (W_Q): Linear(in_features=100, out_features=64, bias=True)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "          )\n",
       "          (linear_layer): Linear(in_features=512, out_features=100, bias=True)\n",
       "        )\n",
       "        (add_and_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward_layer): FeedForward(\n",
       "          (fc1): Linear(in_features=100, out_features=2048, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=2048, out_features=100, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (linear_layer): Linear(in_features=100, out_features=200000, bias=True)\n",
       "    (W_K): Linear(in_features=100, out_features=64, bias=True)\n",
       "    (W_V): Linear(in_features=100, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.98\n",
    "epsilon = 10**(-9)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=3e-5, betas=(beta_1, beta_2), eps=epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpemb_instance_target = transformer_dataset.return_bpemb_target_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\n",
      "=== Model Evaluation ===\n",
      "\n",
      "Sentence 1:\n",
      "Predicted: glitzer cement filmversion trockental offiziers uli at ## at ## at ## at , ,     . . mattei mattei zeller punks  mercantile fette   erstraße lill hynchus . kok sonderburg  eigentl    \n",
      "Actual   :  iron cement ist eine gebrauchs ## at ## - ## at ## fertige paste , die mit einem spachtel oder den fingern als hohlkehle in die forme cken ( winkel ) der stahlguss - kok ille aufgetragen wird .  \n",
      "\n",
      "Sentence 2:\n",
      "Predicted: der der lenzburg schützt iron cement die verwurzelung ille  den sbuch ,  iven  .   \n",
      "Actual   :  nach der aushärtung schützt iron cement die kok ille gegen den heissen , abras iven stahlguss .  \n",
      "\n",
      "Sentence 3:\n",
      "Predicted: feuer fester reparatur kitt für feuer sibirischen ,  ,  , etc    \n",
      "Actual   :  feuer fester reparatur kitt für feuer ungsanlagen , öfen , offene feuerstellen etc .  \n",
      "\n",
      "Sentence 4:\n",
      "Predicted: der bau und scharnier reparatur der auto straßen ...   \n",
      "Actual   :  der bau und die reparatur der auto straßen ...  \n",
      "\n",
      "Sentence 5:\n",
      "Predicted: die mitteilungen sollen den geschäftlichen kommerziellen charakter tragen .   \n",
      "Actual   :  die mitteilungen sollen den geschäftlichen kommerziellen charakter tragen .  \n",
      "\n",
      "Sentence 6:\n",
      "Predicted: der vertrieb schaltgetriebe offiziers und dienst teppichen kunstgeschichte das jard ## at  at ## at  at   dreamworks .   \n",
      "Actual   :  der vertrieb ihrer waren und dienst leistungen durch das postfach ## at ## - ## at ## system wird nicht zugelassen .  \n",
      "\n",
      "Sentence 7:\n",
      "Predicted: die werbe vers pinn (  ) fk die salom korrekte informationen werden gelöscht .   \n",
      "Actual   :  die werbe vers ande ( spam ) und andere un korrekte informationen werden gelöscht .  \n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "\n",
      "=== Model Evaluation ===\n",
      "\n",
      "Sentence 1:\n",
      "Predicted: der cement filmversion eine reparatur ## at ## at ## at ## at , ,  ## volver volver welse mineralen mattei mattei spurius orst die forme cken   ) herning stahlguss . kok ille aufgetragen wird .   \n",
      "Actual   :  iron cement ist eine gebrauchs ## at ## - ## at ## fertige paste , die mit einem spachtel oder den fingern als hohlkehle in die forme cken ( winkel ) der stahlguss - kok ille aufgetragen wird .  \n",
      "\n",
      "Sentence 2:\n",
      "Predicted: der der aushärtung schützt iron cement die kok ille gegen den heissen ,  iven stahlguss .   \n",
      "Actual   :  nach der aushärtung schützt iron cement die kok ille gegen den heissen , abras iven stahlguss .  \n",
      "\n",
      "Sentence 3:\n",
      "Predicted: feuer fester reparatur kitt für feuer ungsanlagen ,  ,  feuerstellen etc    \n",
      "Actual   :  feuer fester reparatur kitt für feuer ungsanlagen , öfen , offene feuerstellen etc .  \n",
      "\n",
      "Sentence 4:\n",
      "Predicted: der bau und die reparatur der auto straßen ...   \n",
      "Actual   :  der bau und die reparatur der auto straßen ...  \n",
      "\n",
      "Sentence 5:\n",
      "Predicted: die mitteilungen sollen den geschäftlichen kommerziellen charakter tragen .   \n",
      "Actual   :  die mitteilungen sollen den geschäftlichen kommerziellen charakter tragen .  \n",
      "\n",
      "Sentence 6:\n",
      "Predicted: der vertrieb ihrer waren und dienst ochte dagmar das postfach ## at ## at ## at ## at kierke nicht zugelassen .   \n",
      "Actual   :  der vertrieb ihrer waren und dienst leistungen durch das postfach ## at ## - ## at ## system wird nicht zugelassen .  \n",
      "\n",
      "Sentence 7:\n",
      "Predicted: die werbe vers ande ( spam ) und andere un korrekte informationen werden gelöscht .   \n",
      "Actual   :  die werbe vers ande ( spam ) und andere un korrekte informationen werden gelöscht .  \n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "\n",
      "=== Model Evaluation ===\n",
      "\n",
      "Sentence 1:\n",
      "Predicted: der cement die eine gebrauchs ## at ## at ## at ## at , ,  mit volver volver welse den mattei als spurius in die forme cken (  ) der stahlguss . kok ille aufgetragen wird .   \n",
      "Actual   :  iron cement ist eine gebrauchs ## at ## - ## at ## fertige paste , die mit einem spachtel oder den fingern als hohlkehle in die forme cken ( winkel ) der stahlguss - kok ille aufgetragen wird .  \n",
      "\n",
      "Sentence 2:\n",
      "Predicted: der der aushärtung schützt iron cement die kok ille gegen den heissen , abras iven stahlguss .   \n",
      "Actual   :  nach der aushärtung schützt iron cement die kok ille gegen den heissen , abras iven stahlguss .  \n",
      "\n",
      "Sentence 3:\n",
      "Predicted: feuer fester reparatur kitt für feuer ungsanlagen ,  ,  feuerstellen etc .   \n",
      "Actual   :  feuer fester reparatur kitt für feuer ungsanlagen , öfen , offene feuerstellen etc .  \n",
      "\n",
      "Sentence 4:\n",
      "Predicted: der bau und die reparatur der auto straßen ...   \n",
      "Actual   :  der bau und die reparatur der auto straßen ...  \n",
      "\n",
      "Sentence 5:\n",
      "Predicted: die mitteilungen sollen den geschäftlichen kommerziellen charakter tragen .   \n",
      "Actual   :  die mitteilungen sollen den geschäftlichen kommerziellen charakter tragen .  \n",
      "\n",
      "Sentence 6:\n",
      "Predicted: der vertrieb ihrer waren und dienst leistungen dagmar das postfach ## at ## at ## at ## at wird nicht zugelassen .   \n",
      "Actual   :  der vertrieb ihrer waren und dienst leistungen durch das postfach ## at ## - ## at ## system wird nicht zugelassen .  \n",
      "\n",
      "Sentence 7:\n",
      "Predicted: die werbe vers ande ( spam ) und andere un korrekte informationen werden gelöscht .   \n",
      "Actual   :  die werbe vers ande ( spam ) und andere un korrekte informationen werden gelöscht .  \n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "\n",
      "=== Model Evaluation ===\n",
      "\n",
      "Sentence 1:\n",
      "Predicted: der cement ist eine gebrauchs ## at ## at ## at ## at paste , ## mit einem spachtel oder den fingern als hohlkehle in die forme cken (  ) der stahlguss . kok ille aufgetragen wird .   \n",
      "Actual   :  iron cement ist eine gebrauchs ## at ## - ## at ## fertige paste , die mit einem spachtel oder den fingern als hohlkehle in die forme cken ( winkel ) der stahlguss - kok ille aufgetragen wird .  \n",
      "\n",
      "Sentence 2:\n",
      "Predicted: nach der aushärtung schützt iron cement die kok ille gegen den heissen , abras iven stahlguss .   \n",
      "Actual   :  nach der aushärtung schützt iron cement die kok ille gegen den heissen , abras iven stahlguss .  \n",
      "\n",
      "Sentence 3:\n",
      "Predicted: feuer fester reparatur kitt für feuer ungsanlagen , öfen , offene feuerstellen etc .   \n",
      "Actual   :  feuer fester reparatur kitt für feuer ungsanlagen , öfen , offene feuerstellen etc .  \n",
      "\n",
      "Sentence 4:\n",
      "Predicted: der bau und die reparatur der auto straßen ...   \n",
      "Actual   :  der bau und die reparatur der auto straßen ...  \n",
      "\n",
      "Sentence 5:\n",
      "Predicted: die mitteilungen sollen den geschäftlichen kommerziellen charakter tragen .   \n",
      "Actual   :  die mitteilungen sollen den geschäftlichen kommerziellen charakter tragen .  \n",
      "\n",
      "Sentence 6:\n",
      "Predicted: der vertrieb ihrer waren und dienst leistungen durch das postfach ## at ## at ## at ## at wird nicht zugelassen .   \n",
      "Actual   :  der vertrieb ihrer waren und dienst leistungen durch das postfach ## at ## - ## at ## system wird nicht zugelassen .  \n",
      "\n",
      "Sentence 7:\n",
      "Predicted: die werbe vers ande ( spam ) und andere un korrekte informationen werden gelöscht .   \n",
      "Actual   :  die werbe vers ande ( spam ) und andere un korrekte informationen werden gelöscht .  \n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "\n",
      "=== Model Evaluation ===\n",
      "\n",
      "Sentence 1:\n",
      "Predicted: der cement ist eine gebrauchs ## at ## at ## at ## at paste ,  mit einem spachtel oder den fingern als hohlkehle in die forme cken (  ) der stahlguss . kok ille aufgetragen wird .   \n",
      "Actual   :  iron cement ist eine gebrauchs ## at ## - ## at ## fertige paste , die mit einem spachtel oder den fingern als hohlkehle in die forme cken ( winkel ) der stahlguss - kok ille aufgetragen wird .  \n",
      "\n",
      "Sentence 2:\n",
      "Predicted: nach der aushärtung schützt iron cement die kok ille gegen den heissen , abras iven stahlguss .   \n",
      "Actual   :  nach der aushärtung schützt iron cement die kok ille gegen den heissen , abras iven stahlguss .  \n",
      "\n",
      "Sentence 3:\n",
      "Predicted: feuer fester reparatur kitt für feuer ungsanlagen , öfen , offene feuerstellen etc .   \n",
      "Actual   :  feuer fester reparatur kitt für feuer ungsanlagen , öfen , offene feuerstellen etc .  \n",
      "\n",
      "Sentence 4:\n",
      "Predicted: der bau und die reparatur der auto straßen ...   \n",
      "Actual   :  der bau und die reparatur der auto straßen ...  \n",
      "\n",
      "Sentence 5:\n",
      "Predicted: die mitteilungen sollen den geschäftlichen kommerziellen charakter tragen .   \n",
      "Actual   :  die mitteilungen sollen den geschäftlichen kommerziellen charakter tragen .  \n",
      "\n",
      "Sentence 6:\n",
      "Predicted: der vertrieb ihrer waren und dienst leistungen durch das postfach ## at ## at ## at ## at wird nicht zugelassen .   \n",
      "Actual   :  der vertrieb ihrer waren und dienst leistungen durch das postfach ## at ## - ## at ## system wird nicht zugelassen .  \n",
      "\n",
      "Sentence 7:\n",
      "Predicted: die werbe vers ande ( spam ) und andere un korrekte informationen werden gelöscht .   \n",
      "Actual   :  die werbe vers ande ( spam ) und andere un korrekte informationen werden gelöscht .  \n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch:\",epoch)\n",
    "    for sentence_index,(source_embedding,target_embedding,target_tokens) in enumerate(transformer_dataset):\n",
    "        logits = transformer(source_embedding, target_embedding)\n",
    "\n",
    "        true_token_probability_distributions = torch.zeros((int(logits.shape[0]), vocab_size))\n",
    "        for current_token_probability_distribution_index in range(0, (len(logits) - 1)):\n",
    "            true_next_token_index = target_tokens[current_token_probability_distribution_index + 1]\n",
    "            true_token_probability_distributions[current_token_probability_distribution_index][true_next_token_index] = 1\n",
    "        # the true next predicted token of the last token should be EOS; my last two token probability distributions will place the highest probability on EOS\n",
    "        EOS_token_index_target_language = bpemb_instance_target.EOS\n",
    "        true_token_probability_distributions[int(logits.shape[0]) - 1][EOS_token_index_target_language] = 1\n",
    "\n",
    "        loss = loss_fn(logits, true_token_probability_distributions)\n",
    "\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # diagnostic prints\n",
    "        size = len(transformer_dataset)\n",
    "        loss, current = loss.item(), sentence_index\n",
    "        # print(f\"sank:loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    if epoch%10==0:\n",
    "        evaluate_model(transformer, transformer_dataset, bpemb_instance_target, vocab_size, device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(transformer, transformer_dataset, bpemb_instance_target, vocab_size, device=\"cpu\"):\n",
    "    \"\"\"Evaluates the Transformer model by printing predictions and actual sentences.\"\"\"\n",
    "    \n",
    "    print(\"\\n=== Model Evaluation ===\")\n",
    "    transformer.eval() \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sentence_index, (source_embedding, target_embedding, target_tokens) in enumerate(transformer_dataset):\n",
    "            logits = transformer(source_embedding.to(device), target_embedding.to(device))\n",
    "            \n",
    "            # Convert logits to predicted token indices\n",
    "            predicted_token_indices = torch.argmax(logits, dim=-1)  # Shape: (seq_len,)\n",
    "\n",
    "            # Decode tokens to words\n",
    "            predicted_sentence = \" \".join(bpemb_instance_target.decode(predicted_token_indices.tolist()))\n",
    "            actual_sentence = \" \".join(bpemb_instance_target.decode(target_tokens))\n",
    "\n",
    "            print(f\"\\nSentence {sentence_index + 1}:\")\n",
    "            print(f\"Predicted: {predicted_sentence}\")\n",
    "            print(f\"Actual   : {actual_sentence}\")\n",
    "    \n",
    "    transformer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
